---
title: "Final Project Scrapes 628"
output: html_document
date: "2023-12-01"
---

```{r setup, include=FALSE}

library(text)

library(reticulate)
library(tidyverse)
library(udpipe)
library(ggwordcloud)
library(rvest)
library(xml2)
library(tidytext)
library(SnowballC)
library(quanteda)
library(text2vec)
library(LDAvis)

```

#Gdelt loop to scrape for Ukraine
```{r}


# create a much simpler function for getting a series of date ranges using the built-in seq.Date function: 
dRangeSeq<-function(start_date, end_date, by=1){
  start_date<-as.Date(start_date)
  end_date<-as.Date(end_date)
  sdates<-seq.Date(start_date, end_date, by=by)
  
  result<-data.frame("start_date" = sdates[-length(sdates)], 
                     "end_date" = sdates[2:length(sdates)])
  return(result)
}
reticulate::py_install("gdeltdoc") 


# import the Filters and GdeltDoc functions from the gdeltdoc python package
py_run_string('from gdeltdoc import GdeltDoc, Filters')




# creating a data frame with start and end dates and getting one day at a time
date_ranges<-dRangeSeq("2022-02-24", "2022-04-24", by=1)

# add a column to track number of articles returned per period
date_ranges$number_returned<-NA

# add a column to track errors
date_ranges$errors<-""

# create an empty vector to count our results:
articles_count <- c()

# create an empty list to contain the actual articles
articles_frame<-data.frame()
i<-1

# for every number from 1 to the number of rows the date_ranges frame, do whatever is in the curly brackets: 
for(i in i:nrow(date_ranges)){
  f = py$Filters(
    # as.character() converts the date to text so python can handle it: 
    start_date = as.character(date_ranges$start_date[i]), 
    end_date = as.character(date_ranges$end_date[i]),
   #end_date = 'sfadf',
    keyword= c('ukraine', 'russia', 'invasion'), 
    country = 'US',
    domain_exact = "apnews.com"
  )

  gd = py$GdeltDoc()
  
  print(i)
  
  # Search for articles matching the filters
  start_time<-Sys.time()
  
  # tryCatch for error handling
  result <- tryCatch({
    
    # retrieve articles
    articles = gd$article_search(f)
   
    # add count to the date ranges frame
    date_ranges$number_returned[i]<-nrow(articles)
    
    # add articles to total
    articles_frame<-bind_rows(articles_frame, articles)%>%
      distinct()
    # print results
    print(sprintf("%s articles retrieved", nrow(articles)))
    
  }, error = function(e) {
    # if there is an error, log it 
    date_ranges$errors[i] <- paste(e)
    # and also print a warning
    print(sprintf('an error occurred for on iteration %s', i))
    }
  
  )
  
  
  end_time<-Sys.time()
  
  # this isn't required, it just lets us know that the loop is running: 
  
  # pauses the system afte reach iteration - slows it down further if the API slows down
  Sys.sleep((end_time - start_time) +1)
  

  
}

# Note: the loop might require some supervision to get everything you want 
# Gdelt only returns up to 250 articles at a time, so ideally you want to keep the list of terms/sources/dates 
#   narrow enough so that you get fewer than 250 with each query. 
# if you're doing a huge number of articles, you might want to do this in batches 


#get rid of dupes
ukraine_frame <- articles_frame%>%
  distinct(title, .keep_all = TRUE)



```

#israel scrape

```{r}



# create a much simpler function for getting a series of date ranges using the built-in seq.Date function: 
dRangeSeq<-function(start_date, end_date, by=1){
  start_date<-as.Date(start_date)
  end_date<-as.Date(end_date)
  sdates<-seq.Date(start_date, end_date, by=by)
  
  result<-data.frame("start_date" = sdates[-length(sdates)], 
                     "end_date" = sdates[2:length(sdates)])
  return(result)
}

# import the Filters and GdeltDoc functions from the gdeltdoc python package
py_run_string('from gdeltdoc import GdeltDoc, Filters')




# creating a data frame with start and end dates and getting one day at a time
date_ranges<-dRangeSeq("2023-10-07", "2023-12-01", by=1)

# add a column to track number of articles returned per period
date_ranges$number_returned<-NA

# add a column to track errors
date_ranges$errors<-""

# create an empty vector to count our results:
articles_count <- c()

# create an empty list to contain the actual articles
articles_frame<-data.frame()
i<-1

# for every number from 1 to the number of rows the date_ranges frame, do whatever is in the curly brackets: 
for(i in i:nrow(date_ranges)){
  f = py$Filters(
    # as.character() converts the date to text so python can handle it: 
    start_date = as.character(date_ranges$start_date[i]), 
    end_date = as.character(date_ranges$end_date[i]),
   #end_date = 'sfadf',
    keyword= c('israel', 'hamas'), 
    country = 'US',
    domain_exact = "apnews.com"
  )
  gd = py$GdeltDoc()
  
  print(i)
  
  # Search for articles matching the filters
  start_time<-Sys.time()
  
  # tryCatch for error handling
  result <- tryCatch({
    
    # retrieve articles
    articles = gd$article_search(f)
   
    # add count to the date ranges frame
    date_ranges$number_returned[i]<-nrow(articles)
    
    # add articles to total
    articles_frame<-bind_rows(articles_frame, articles)%>%
      distinct()
    # print results
    print(sprintf("%s articles retrieved", nrow(articles)))
    
  }, error = function(e) {
    # if there is an error, log it 
    date_ranges$errors[i] <- paste(e)
    # and also print a warning
    print(sprintf('an error occurred for on iteration %s', i))
    }
  
  )
  
  
  end_time<-Sys.time()
  
  # this isn't required, it just lets us know that the loop is running: 
  
  # pauses the system afte reach iteration - slows it down further if the API slows down
  Sys.sleep((end_time - start_time) +1)
  

  
}

# Note: the loop might require some supervision to get everything you want 
# Gdelt only returns up to 250 articles at a time, so ideally you want to keep the list of terms/sources/dates 
#   narrow enough so that you get fewer than 250 with each query. 
# if you're doing a huge number of articles, you might want to do this in batches 


#get rid of dupes
israel_frame <- articles_frame%>%
  distinct(title, .keep_all = TRUE)
write.csv(israel_frame, file="israel_frame.csv")


```
#israel body text
```{r}

length(israel_frame$title)
  
for(i in 1:length(israel_frame$title)){
  webpage <- xml2::read_html(israel_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  israel_frame$bodytext[i]<-bodytext
  
} 



  
write.csv(israel_frame, file="israelframe.csv")

```
#body text ukraine
```{r}
length(ukraine_frame$title)
  
for(i in 1:500){
  webpage <- xml2::read_html(ukraine_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  ukraine_frame$bodytext[i]<-bodytext
  
} 

for(i in 500:1000){
  webpage <- xml2::read_html(ukraine_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  ukraine_frame$bodytext[i]<-bodytext
  print(i)
} 

for(i in 1001:1500){
  webpage <- xml2::read_html(ukraine_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  ukraine_frame$bodytext[i]<-bodytext
  print(i)
} 

for(i in 1501:2000){
  webpage <- xml2::read_html(ukraine_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  ukraine_frame$bodytext[i]<-bodytext
  print(i)
} 

for(i in 2001:length(ukraine_frame$url)){
  webpage <- xml2::read_html(ukraine_frame$url[i])
  bodytext <- webpage %>%
    html_elements(css = '.RichTextStoryBody') %>%
    html_text()%>%
    str_flatten()%>%
    str_squish()
  ukraine_frame$bodytext[i]<-bodytext
  print(i)
} 
ukraine_frame <- ukraine_frame%>%
  distinct(bodytext, .keep_all = TRUE)
  
write.csv(ukraine_frame, file="ukraineframe.csv")
```
#sentiment analysis israel

##Extracts name of leader,

###descriptive stats of common words that show up around that leaders name

###compiles average sentiment of those words


```{r}
text <- read.csv('israelframe.csv')

# extract only paragraphs that mention Important entities

netanyahutest<-text%>%
  filter(str_detect(bodytext, 'Netanyahu'))

netanyahutest<-text%>%
  filter(str_detect(bodytext, '\\bNetanyahu\\b'))



tokens<-text%>%
  unnest_tokens(input = bodytext, token='words', output='word')






head(tokens$word)
head(stop_words, n=15)
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)

tokens_counts%>%
  arrange(-n)%>%
  
  slice_head(n=20)%>%
  
  ggplot(aes(x=reorder(word, n), y=n))+ 
  geom_bar(stat='identity')+
  coord_flip() +
  theme_bw()

tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()

get_sentiments('bing')%>%
  head()


tone<-tokens%>% 
  #keep only tokens that are in the sentiment dictionary
  inner_join(get_sentiments('bing'))%>%
  group_by(url)%>%
  summarise(n = n(),
            positive= sum(sentiment == 'positive'),
            negative = sum(sentiment =='negative'),
            average_tone_is= (positive-negative)/n
  )


ggplot(tone, aes(x=average_tone_is)) + 
  geom_histogram() +
  theme_bw()

compiled_sent_is <- sum(tone$average_tone_is)
compiled_sent_is
library(SnowballC)

tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  summarize(n = n())%>%
  arrange(-n)%>%
  slice_head(n=15)
#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)


topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity') +
  theme_bw() +
  coord_flip()

udpipe_download_model("english")

english_model <- udpipe_load_model('english-ewt-ud-2.5-191206.udpipe')
parsed_text <- udpipe_annotate(object=english_model, x = text$bodytext[1:3], doc_id = text$url[1:3], trace=2)

parsed_israel <- data.frame(parsed_text)
```
#sentiment analysis ukraine

##Extracts name of leader,

###descriptive stats of common words that show up around that leaders name

###compiles average sentiment of those words
```{r}
text <- read.csv('ukraineframe.csv')

# extract only paragraphs that mention Important entities

putintest<-text%>%
  filter(str_detect(bodytext, 'Putin'))

putintest<-text%>%
  filter(str_detect(bodytext, '\\bPutin\\b'))



tokens<-text%>%
  unnest_tokens(input = bodytext, token='words', output='word')






head(tokens$word)
head(stop_words, n=15)
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)

tokens_counts%>%
  arrange(-n)%>%
  
  slice_head(n=20)%>%
  
  ggplot(aes(x=reorder(word, n), y=n))+ 
  geom_bar(stat='identity')+
  coord_flip() +
  theme_bw()

tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()

get_sentiments('bing')%>%
  head()


tone<-tokens%>% 
  #keep only tokens that are in the sentiment dictionary
  inner_join(get_sentiments('bing'))%>%
  group_by(url)%>%
  summarise(n = n(),
            positive= sum(sentiment == 'positive'),
            negative = sum(sentiment =='negative'),
            average_tone_uk <-  (positive-negative)/n
  )



ggplot(tone, aes(x=average_tone_uk)) + 
  geom_histogram() +
  theme_bw()
compiled_sent_uk <- sum(tone$`average_tone_uk <- (positive - negative)/n`) 
compiled_sent_uk

library(SnowballC)

tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  summarize(n = n())%>%
  arrange(-n)%>%
  slice_head(n=15)
#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)


topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity') +
  theme_bw() +
  coord_flip()

udpipe_download_model("english")

english_model <- udpipe_load_model('english-ewt-ud-2.5-191206.udpipe')
parsed_text <- udpipe_annotate(object=english_model, x = text$bodytext[1:3], doc_id = text$url[1:3], trace=2)

parsed_ukraine <- data.frame(parsed_text)

```
#Intertopic Distance Map Ukraine

#This uses the LDAvis package to create a visual representation of the most common topics

#It uses machine learning to put 'alike' words into buckets

#This is set up to create 10 sperate topics/buckets, but sometimes the topics are a bit wonky
```{r}


poldf<-read_csv('ukraineframe.csv')
# note we use with and the . operator to make this command work with tidy commands


pol_corpus<-poldf%>%
  with(.,corpus(bodytext,
                docvars = data.frame(url, title, bodytext)))

# tokenize texts
pol_tokens<-tokens(pol_corpus,
                   what = 'word',
                   remove_punct =TRUE,
                   remove_numbers =TRUE,
                   remove_symbols= TRUE,
                   include_docvars = TRUE
)


#keywords in context
kw_putin <- kwic(pol_tokens, pattern= "Putin")
head(kw_putin, 10)


##cleaning
toks_nostop <- tokens_select(pol_tokens, pattern=stopwords("en"), selection="remove")
pol_dfm <- dfm(toks_nostop)
topfeatures(pol_dfm)
pol_dfm


#remove features that occur less than 10 times
pol_dfm <- dfm_trim(pol_dfm, min_termfreq = 10)

#remove features that occur in more than 10% of douments
pol_dfm <- dfm_trim(pol_dfm, max_docfreq=0.1, docfreq_type = "prop")

pol_dfm

topfeatures(pol_dfm, 10)
policyAgendas<-readRDS(url('https://github.com/Neilblund/APAN/raw/main/policy_agendas_dictionary.rds'))

policyagendas.dict <- dictionary(policyAgendas)
agenda_politics <- dfm_lookup(pol_dfm, dictionary = policyagendas.dict, levels = 1)



###topic modeling ---
lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
  lda_model$fit_transform(x = pol_dfm, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)
lda_model$get_top_words(n = 10, topic_number = c(1:10), lambda = 1)

lda_model$plot()


```
#Intertopic Distance Map Israel

#This uses the LDAvis package to create a visual representation of the most common topics

#It uses machine learning to put 'alike' words into buckets

#This is set up to create 10 sperate topics/buckets, but sometimes the topics are a bit wonky
```{r}


poldf<-read_csv('israelframe.csv')
# note we use with and the . operator to make this command work with tidy commands


pol_corpus<-poldf%>%
  with(.,corpus(bodytext,
                docvars = data.frame(url, title, bodytext)))

# tokenize texts
pol_tokens<-tokens(pol_corpus,
                   what = 'word',
                   remove_punct =TRUE,
                   remove_numbers =TRUE,
                   remove_symbols= TRUE,
                   include_docvars = TRUE
)



##cleaning
toks_nostop <- tokens_select(pol_tokens, pattern=stopwords("en"), selection="remove")
pol_dfm <- dfm(toks_nostop)
topfeatures(pol_dfm)
pol_dfm


#remove features that occur less than 10 times
pol_dfm <- dfm_trim(pol_dfm, min_termfreq = 10)

#remove features that occur in more than 10% of douments
pol_dfm <- dfm_trim(pol_dfm, max_docfreq=0.1, docfreq_type = "prop")

pol_dfm

topfeatures(pol_dfm, 10)
policyAgendas<-readRDS(url('https://github.com/Neilblund/APAN/raw/main/policy_agendas_dictionary.rds'))

policyagendas.dict <- dictionary(policyAgendas)
agenda_politics <- dfm_lookup(pol_dfm, dictionary = policyagendas.dict, levels = 1)



###topic modeling ---
lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr =
  lda_model$fit_transform(x = pol_dfm, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)
lda_model$get_top_words(n = 10, topic_number = c(1:10), lambda = 1)

lda_model$plot()
```